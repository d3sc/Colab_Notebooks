{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QXbO_BHLwDGn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700571565701,"user_tz":-420,"elapsed":77002,"user":{"displayName":"Vin","userId":"04719221619972503905"}},"outputId":"519c465f-33ff-4f0a-ac97-74931a71adf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  7883    0  7883    0     0  16067      0 --:--:-- --:--:-- --:--:-- 16186\n",">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",">>> The Ollama API is now available at 0.0.0.0:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.8/731.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hpulling manifest\n","pulling 6ae280299950... 100% ▕▏ (4.1/4.1 GB, 197 MB/s)\n","pulling 22e1b2e8dc2f... 100% ▕▏ (43/43 B, 33 B/s)\n","pulling e35ab70a78c7... 100% ▕▏ (90/90 B, 71 B/s)\n","pulling 1cb90d66f4d4... 100% ▕▏ (381/381 B, 254 B/s)\n","verifying sha256 digest\n","writing manifest\n","removing any unused layers\n","success\n"]}],"source":["!curl https://ollama.ai/install.sh | sh\n","\n","import subprocess\n","subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n","\n","!pip install pyngrok -q\n","!pip install pypi-json -q\n","!pip install flask-cors -q\n","!ollama pull mistral"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iDeVxi6BPi7y","executionInfo":{"status":"ok","timestamp":1700571568955,"user_tz":-420,"elapsed":14,"user":{"displayName":"Vin","userId":"04719221619972503905"}}},"outputs":[],"source":["import json\n","import subprocess\n","from pyngrok import ngrok\n","from flask import Flask, request\n","import requests\n","from flask_cors import CORS"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"JcytlkOlFDcE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700571571303,"user_tz":-420,"elapsed":998,"user":{"displayName":"Vin","userId":"04719221619972503905"}},"outputId":"a3308590-e829-4a2e-859b-e35ac5534b10"},"outputs":[{"output_type":"stream","name":"stdout","text":[]}],"source":["ngrok.set_auth_token(\"2NPZbjWnWpqUallvFrDj3Q6z8MQ_32PYSNp438wFuskyvMs9M\")\n","ollama_url = 'http://localhost:11434'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98IVaHIy4M0_","outputId":"62eee050-c380-4053-b7d3-f79cfb73ddca"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2023-11-21T13:19:11+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"]},{"output_type":"stream","name":"stdout","text":["NgrokTunnel: \"https://7469-34-118-242-211.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:19:40] \"POST /api HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:20:53] \"POST /api HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:22:52] \"POST /api HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:33:54] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:34:19] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:34:49] \"POST /api HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:37:13] \"\u001b[31m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 405 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:37:39] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:40:39] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [21/Nov/2023 13:47:19] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n"]}],"source":["app = Flask(__name__)\n","CORS(app)\n","\n","# main Route\n","@app.route('/')\n","def index():\n","    !curl http://localhost:11434/api/generate -d '{\"model\": \"mistral\",\"prompt\":\"Why is the sky blue?\"}'\n","    return json.dumps({'Status': 'OK', 'message': 'Server ready to Use!'})\n","\n","# Api handle Route\n","@app.route(\"/api\", methods=[\"GET\",\"POST\"])\n","def api_handle():\n","\n","# if with http POST body\n","  data = request.json\n","  model = data[\"model\"]=False if data.get(\"model\") == None else data[\"model\"]\n","  prompt = data[\"prompt\"]\n","\n","\n","# if with query parameter\n","  # prompt = request.args.get(\"prompt\")\n","  # model = request.args.get(\"model\")\n","\n","  json_data = {\n","      \"model\": model or \"mistral\",\n","      \"prompt\": prompt,\n","      \"stream\": False,\n","      \"system\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"\n","  }\n","  headers = { \"Content-Type\": \"application/json\" }\n","  response = requests.post(f\"{ollama_url}/api/generate\", json=json_data, headers=headers)\n","  return response.json()[\"response\"] + \"\\n\"\n","\n","# main func\n","def main() :\n","  subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n","  print(ngrok.connect(5000))\n","  app.run()\n","\n","if __name__ == \"__main__\" :\n","  try:\n","    main()\n","  except KeyboardInterrupt:\n","    print(\"stop working!!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhRUqFqZdFfJ"},"outputs":[],"source":["# run with curl\n","# curl -H \"Content-Type: application/json\" -X POST -d '{\"prompt\": \"hello friend, what is your name?\"}' https://6111-34-123-21-132.ngrok-free.app/api"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNu20KlNpYwI/qRR3lbxnnw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}